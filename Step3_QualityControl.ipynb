{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oOk8iv-DogA"
      },
      "source": [
        "# Step 3: Quality Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qNLP5aMDogI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KJHsorLsEPiy",
        "outputId": "2a40ada3-7595-4273-e108-4f640add1c5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iIRRELwDogJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from spellchecker import SpellChecker\n",
        "from master_functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "hi1Dgp2PEcLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykoWrWEFDogM"
      },
      "outputs": [],
      "source": [
        "# Suppress false positive warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwKFhruMDogN"
      },
      "source": [
        "#### Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dz_4MQyDogO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/lisavirginia/clinical-abbreviations/master/code/Step2Output.csv',\n",
        "                 sep='|',\n",
        "                 header=0,\n",
        "                 index_col=False,\n",
        "                 na_filter=False,\n",
        "                 dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx6Tj4vLDogP"
      },
      "outputs": [],
      "source": [
        "df.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aMsPCP6DogP"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5BUt1iMDogQ"
      },
      "source": [
        "## Identify Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSeQJjv9DogR"
      },
      "source": [
        "#### Heuristic 1: Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gygxv6cRDogR"
      },
      "source": [
        "Identify which records exactly duplicate another record from the same source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpxDXp_gDogS"
      },
      "outputs": [],
      "source": [
        "Extract1 = df[df.duplicated(['SF', 'LF', 'Source']) == True]\n",
        "Extract1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5d6ejDDogT"
      },
      "source": [
        "#### Heuristic 2: Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhRggFPDogT"
      },
      "source": [
        "Identify excess punctuation in the long form (e.g. \"nitric oxide;\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1KWsNJjDogU"
      },
      "outputs": [],
      "source": [
        "# Punctuation after LF (excludes .+%()[])\n",
        "Extract2_1 = df[df['LF'].str.contains('.*[,\\/#!\\$\\^&@\\?<>\\*:;{}=\\-_\\'~\\\"]$') == True]\n",
        "Extract2_1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOPcxKSnDogU"
      },
      "outputs": [],
      "source": [
        "# Punctuation before LF (excludes .+%()[])\n",
        "Extract2_2 = df[df['LF'].str.contains('^[,\\/#!\\$\\^&@\\?<>\\*:;{}=\\-_\\'~\\\"].*') == True]\n",
        "Extract2_2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPN7QjaVDogV"
      },
      "source": [
        "Identify excess punctuation in the short form (e.g. \"..IVF\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faiHWn09DogV"
      },
      "outputs": [],
      "source": [
        "# Excess periods before SF\n",
        "Extract2_3 = df[df['SF'].str.contains('^[\\.]+.*') == True]\n",
        "Extract2_3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-oPCeVADogW"
      },
      "source": [
        "#### Heuristic 3: Spelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbQvcthBDogW"
      },
      "source": [
        "The long form contains spelling errors (e.g. \"cncer\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16neyVnzDogW"
      },
      "outputs": [],
      "source": [
        "# Set spell checker parameters\n",
        "spell = SpellChecker(distance=1)\n",
        "\n",
        "# Add medical word corpus (UMLS Metathesaurus)\n",
        "spell.word_frequency.load_text_file('data/ClinSpell.txt')\n",
        "\n",
        "# Exclude UMLS and ADAM\n",
        "subset = df[(df['Source'] != 'UMLS') &\n",
        "            (df['Source'] != 'ADAM')]\n",
        "\n",
        "# Instantiate output\n",
        "misspelled_rows = []\n",
        "misspelled_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUL_Wu2ODogX"
      },
      "outputs": [],
      "source": [
        "# Iterate over subset\n",
        "for index, row in subset.iterrows():\n",
        "\n",
        "    # Format LF for spellchecker\n",
        "    pre_token = re.sub('[^A-Za-z\\s\\-]+', '', row['LF']).lower()\n",
        "    token = list(filter(None, re.split(r'[\\s\\-]+', pre_token)))\n",
        "\n",
        "    # Identify misspelled LFs\n",
        "    misspelled = spell.unknown(token)\n",
        "    if len(misspelled) > 0:\n",
        "        misspelled_rows.append(row['RecordID'])\n",
        "        misspelled_data.append(misspelled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRnnDcGpDogX"
      },
      "outputs": [],
      "source": [
        "# Extract misspelled LFs\n",
        "Extract3 = df[df['RecordID'].isin(misspelled_rows)]\n",
        "Extract3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypsJ7fDlDogX"
      },
      "source": [
        "#### Heuristic 4: Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_HisvbDogY"
      },
      "source": [
        "The alphabetic characters in the short form don't occur anywhere in the long form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keMnfNfrDogY"
      },
      "outputs": [],
      "source": [
        "# Include problematic sources\n",
        "subset = df[(df['Source'] == 'Vanderbilt Clinic Notes') |\n",
        "            (df['Source'] == 'Vanderbilt Discharge Sums')]\n",
        "\n",
        "# Instantiate output\n",
        "missing_character = []\n",
        "missing_char_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdbZAeZgDogY"
      },
      "outputs": [],
      "source": [
        "# Iterate over dataframe\n",
        "for index, row in subset.iterrows():\n",
        "\n",
        "    # Extract alphabetic characters\n",
        "    alph_SF = set(re.sub('[^A-Za-z]+', '', row['SF']).lower())\n",
        "    alph_LF = set(re.sub('[^A-Za-z]+', '', row['LF']).lower())\n",
        "\n",
        "    if alph_SF.issubset(alph_LF) == False:\n",
        "        if (alph_SF - alph_LF) != {'x'}:\n",
        "            missing_character.append(row['RecordID'])\n",
        "            missing_char_data.append(alph_SF - alph_LF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKsSM3N4DogZ"
      },
      "outputs": [],
      "source": [
        "# Extract LFs missing characters\n",
        "Extract4 = df[df['RecordID'].isin(missing_character)]\n",
        "Extract4.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLx3ESluDogZ"
      },
      "source": [
        "#### Heuristic 5: User-Identified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqhofdhADogZ"
      },
      "outputs": [],
      "source": [
        "Extract5 = df[(df['LF'].str.contains(\"#000066\") |\n",
        "              df['LF'].str.contains(\"typo\") |\n",
        "              df['LF'].str.contains(\"not an abbreviation\") |\n",
        "              df['LF'].str.contains(\"not an acronym\"))]\n",
        "Extract5.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrO_UnKvDoga"
      },
      "source": [
        "## Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZT9ON5wDoga"
      },
      "source": [
        "#### Add Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbAgbxDtDoga"
      },
      "outputs": [],
      "source": [
        "# Error type, decision, modification\n",
        "Extract1['error'], Extract1['action'] = [\"duplicate\", \"retire\"]\n",
        "Extract2_1['error'], Extract2_1['action'] = [\"punctuation after LF\", \"modify\"]\n",
        "Extract2_2['error'], Extract2_2['action'] = [\"punctuation before LF\", \"modify\"]\n",
        "Extract2_3['error'], Extract2_3['action'] = [\"punctuation before SF\", \"modify\"]\n",
        "Extract3['error'], Extract3['action'] = [misspelled_data, \"modify\"]\n",
        "Extract4['error'], Extract4['action'] = [missing_char_data, \"modify\"]\n",
        "Extract5['error'], Extract5['action'] = [\"user identified\", \"retire\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ75WLKuDogb"
      },
      "source": [
        "#### Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8zCLndQDogb"
      },
      "outputs": [],
      "source": [
        "errors = pd.concat([Extract1, Extract2_1, Extract2_2, Extract2_3, Extract3, Extract4, Extract5])\n",
        "errors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE93AvRtDogb"
      },
      "outputs": [],
      "source": [
        "errors = errors.drop_duplicates(subset=\"RecordID\")\n",
        "errors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TkaQS2Dogb"
      },
      "source": [
        "#### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0-H272hDogb"
      },
      "outputs": [],
      "source": [
        "errors.to_csv('data/Errors_Automated.csv',\n",
        "              index=False,\n",
        "              header=True,\n",
        "              sep='|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoRRiE18Dogc"
      },
      "source": [
        "## Import Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhUS24GGDogc"
      },
      "source": [
        "#### Import Annotated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJFsfSiDogd"
      },
      "outputs": [],
      "source": [
        "errors = pd.read_csv('data/Errors_Annotated.csv',\n",
        "                     sep='|',\n",
        "                     header=0,\n",
        "                     index_col=False,\n",
        "                     na_filter=False,\n",
        "                     dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yndjtb8NDogd"
      },
      "outputs": [],
      "source": [
        "errors.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6fumCV5Dogd"
      },
      "outputs": [],
      "source": [
        "errors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWK87uheDogd"
      },
      "outputs": [],
      "source": [
        "errors['action'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPaL6wF7Doge"
      },
      "source": [
        "#### Remove None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74jSZ6dxDoge"
      },
      "outputs": [],
      "source": [
        "errors = errors[(errors['action'] != 'none')]\n",
        "errors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpVwFDIxDogf"
      },
      "source": [
        "#### Subset Crosswalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6sdnTxDogf"
      },
      "outputs": [],
      "source": [
        "df_all = df # Keep unsubsetted version\n",
        "df = df[~df['RecordID'].isin(errors['RecordID'])]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73HMCEnDogf"
      },
      "source": [
        "#### Subset Errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaR8E2V5Dogg"
      },
      "outputs": [],
      "source": [
        "retire = df_all[df_all['RecordID'].isin(errors[(errors['action'] == 'retire')]['RecordID'])]\n",
        "retire.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFIP8doYDogg"
      },
      "outputs": [],
      "source": [
        "modify = errors[(errors['action'] == 'modify')].iloc[:, 0:19]\n",
        "modify.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrnLK4zVDogg"
      },
      "source": [
        "## Modify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvRTeqAvDogh"
      },
      "source": [
        "#### Retire Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9-cD7kYDogh"
      },
      "outputs": [],
      "source": [
        "# Identify duplicates\n",
        "dups = pd.concat([df, modify])\n",
        "dups = dups[dups.duplicated(['SF', 'LF', 'Source']) == True]\n",
        "dups.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyH75AoGDogi"
      },
      "outputs": [],
      "source": [
        "# Remove from modify\n",
        "modify = modify[~modify['RecordID'].isin(dups['RecordID'])]\n",
        "modify = modify.reset_index(drop=True)\n",
        "modify.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOZCMipzDogi"
      },
      "outputs": [],
      "source": [
        "# Add to retire\n",
        "retire = pd.concat([retire, df_all[df_all['RecordID'].isin(dups['RecordID'])]])\n",
        "retire = retire.reset_index(drop=True)\n",
        "retire.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMkHBBB2Dogj"
      },
      "source": [
        "#### Strip Source Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXJsJ7GODogj"
      },
      "source": [
        "This is done as the source data is potentially no longer valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-nRX2C6Dogk"
      },
      "outputs": [],
      "source": [
        "modify['SFUI'], modify['NormSF'], modify['NSFUI'], modify['PrefSF'] = ['', '', '', '']\n",
        "modify['LFUI'], modify['NormLF'], modify['PrefLF'], modify['SFEUI'] = ['', '', '', '']\n",
        "modify['LFEUI'], modify['Type'], modify['Score'], modify['Count'] = ['', '', '', '']\n",
        "modify['Frequency'], modify['UMLS.CUI'] = ['', '']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5jKkaPKDogk"
      },
      "outputs": [],
      "source": [
        "modify.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d3qXXQsDogk"
      },
      "source": [
        "#### Reassign Normalized Short Form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DhxWoQUDogl"
      },
      "outputs": [],
      "source": [
        "modify['NormSF'] = modify['SF'].apply(normalized_short_form)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6rgR7IPDogl"
      },
      "outputs": [],
      "source": [
        "modify.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Eg0C7gDogm"
      },
      "source": [
        "#### Reassign SFUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UlxJuLrDogm"
      },
      "outputs": [],
      "source": [
        "# Search existing SFUIs\n",
        "for index, row in modify.iterrows():\n",
        "    temp = df_all[(df_all['SF'] == modify['SF'].iat[index])]\n",
        "    if temp.empty:\n",
        "        modify['SFUI'].iat[index] = ''\n",
        "    else:\n",
        "        modify['SFUI'].iat[index] = temp.iloc[0]['SFUI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8lMIf40Dogm"
      },
      "outputs": [],
      "source": [
        "# If none, add SFUI\n",
        "modify = add_new_SFUI(modify)\n",
        "modify.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma7RLLYuDogn"
      },
      "source": [
        "#### Reassign LFUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swkzXdIoDogn"
      },
      "outputs": [],
      "source": [
        "# Search existing LFUIs\n",
        "for index, row in modify.iterrows():\n",
        "    temp = df_all[(df_all['LF'] == modify['LF'].iat[index])]\n",
        "    if temp.empty:\n",
        "        modify['LFUI'].iat[index] = ''\n",
        "    else:\n",
        "        modify['LFUI'].iat[index] = temp.iloc[0]['LFUI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOT_yCzTDogn"
      },
      "outputs": [],
      "source": [
        "# If none, add LFUI\n",
        "modify = add_new_LFUI(modify)\n",
        "modify.sample(3, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Extraction\n",
        "Here We are using the keyword extraction as in natural language processing (NLP), keyword extraction serves a variety of purposes, including: Document Summary: Summarize content by identifying and extracting important topics.\n",
        "\n",
        "Information Retrieval:  Improve search engine performance by indexing and retrieving documents based on  keywords.\n",
        "Content Tagging: Categorize and tag content for better organization and structure.\n",
        "Content Recommendations: Improve personalized content recommendations by analyzing user requests.\n",
        "Social Media Monitoring:  Monitor trends, sentiment, and popular topics on  social media platforms.\n",
        "SEO: Optimize your web content by identifying and incorporating relevant keywords to improve search engine rankings.\n",
        "Market Research: Analyze customer feedback and opinions to understand market trends.\n",
        "Legal Analytics: Extract keywords from  legal documents for faster analysis and compliance monitoring.\n",
        "Text Clustering:  Group similar documents based on  extracted keywords.\n",
        "Question Answering: Improve the accuracy of question answering systems by identifying relevant keywords.\n",
        "Healthcare Text Mining: Extract keywords from biomedical literature and medical records for research and analysis.\n",
        ""
      ],
      "metadata": {
        "id": "-jcueo3CEtHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "gsBuxINeF0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords(df):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(df)\n",
        "    stop_words = set(stopwords.words('english')) #Tokenizing each sentence into words\n",
        "    words = [word.lower() for sentence in sentences for word in word_tokenize(sentence) if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = FreqDist(words) #This calculates the Frequency distribution on the words\n",
        "    keywords = [word for word, freq in word_freq.items() if freq > 1]\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "uTLEG2UXF6c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example text (replace this with your actual text)\n",
        "    text = \"\"\"\n",
        "    Hey there! I've been diving into Natural Language Processing (NLP), which is this super cool field in artificial intelligence all about how computers and humans can chat using regular language. NLP techniques help computers analyze, understand, and even generate human language in a way that makes sense and fits the context.\n",
        "    I'm particularly excited about keyword extraction â€“ it's this really important job in NLP. What it does is figure out the most important words or phrases in a piece of text. \"\"\"\n",
        "\n",
        "    keywords = extract_keywords(text)  # Extracting keywords from the example text\n",
        "    print(\"Keywords:\", keywords)     # Printing the extracted keywords\n"
      ],
      "metadata": {
        "id": "mlBWSYaBGZXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg32AIP0Dogo"
      },
      "source": [
        "#### Add \"Modified\" Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7A-TYq4Dogo"
      },
      "outputs": [],
      "source": [
        "modify[\"Modified\"] = \"modified\"\n",
        "df[\"Modified\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3NzPIEMDogp"
      },
      "source": [
        "#### Append to Crosswalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-N7SDTpDogp"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df, modify])\n",
        "df = df.sort_values(by=['RecordID'])\n",
        "df = df.reset_index(drop=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zesFuX0TDogq"
      },
      "source": [
        "## Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyRCWmAIDogq"
      },
      "source": [
        "#### Export Modify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCpiTheLDogq"
      },
      "outputs": [],
      "source": [
        "# Get original rows\n",
        "modify = df_all[df_all['RecordID'].isin(modify['RecordID'])]\n",
        "modify.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ByI9av3Dogq"
      },
      "outputs": [],
      "source": [
        "modify.to_csv('ModifiedRecords.csv',\n",
        "              index=False,\n",
        "              header=True,\n",
        "              sep='|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-qICkWcDogr"
      },
      "source": [
        "#### Export Retire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETBDLonuDogr"
      },
      "outputs": [],
      "source": [
        "retire.to_csv('RetiredRecords.csv',\n",
        "              index=False,\n",
        "              header=True,\n",
        "              sep='|')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXjSTOkZDogr"
      },
      "source": [
        "#### Export Crosswalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqyyKkAHDogr"
      },
      "outputs": [],
      "source": [
        "df.to_csv('Step3Output.csv',\n",
        "          index=False,\n",
        "          header=True,\n",
        "          sep='|')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (venv_py37_full)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}